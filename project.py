# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AZXvGk_VTu4hMB-zhntWToO1Q3FfhgsW

# **Portfolio Risk Analysis Using Black-Scholes and Copula Models**

### **Team Members** : Laura Bayle, Yan Fu, Vasilii Nosov, Erica Zhao

## **Project Overview**
This project evaluates the risk associated with a long butterfly spread options portfolio using SPX and VIX indices as risk factors. The primary goal is to compute the one-day 95% Value at Risk (VaR) for the portfolio. The analysis is based on historical data, Monte Carlo simulations, and dependency modeling techniques, with the Black-Scholes option pricing model serving as the core pricing method.

### **Butterfly Spread Strategy**

The risk analysis incorporates advanced financial techniques, including t-Copula modeling, Cholesky decomposition, and implied volatility estimation from the VIX index. The results provide insights into portfolio behavior under various market conditions.

## **Table of Contents**

**Part 1: Data Download and Preprocessing**  - Historical SPX and VIX data collection and log return computation  
**Part 2: Statistical Analysis**  - Descriptive statistics and correlation analysis  
**Part 3: Visualizations**  - Log return trends and scatter plots  
**Part 4: Market Scenarios via Copula Modeling**  - t-Copula modeling, eCDF transformation, and scenario simulation  
**Part 5: Options Pricing Using Black-Scholes Model**  - Monte Carlo simulations and portfolio VaR computation

# Part 1: Data Download and Preprocessing
"""

################ t-copula for risk-factors ################

# Import libraries
import yfinance as yf
import scipy.stats as stats
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import t, norm
from statsmodels.distributions.copula.api import StudentTCopula
from statsmodels.distributions.copula.copulas import CopulaDistribution
from scipy.stats import t, kurtosis
import math
import xlrd
import scipy as sp
from scipy.optimize import minimize_scalar
from scipy.stats import spearmanr
import seaborn as sns

# Set fixed date range
# start='2023-10-10', end='2024-10-10'

start_date = '2018-03-26'
end_date = '2019-03-26'

# Download data for SPX and VIX

spx_data = yf.download('^SPX', start=start_date, end=end_date, interval='1d')
vix_data = yf.download('^VIX', start=start_date, end=end_date, interval='1d')

# Reset index to get 'Date' as a column
spx_data.reset_index(inplace=True)
vix_data.reset_index(inplace=True)

# Rename columns for clarity
spx_data.rename(columns={'Close': 'SPX_Level'}, inplace=True)
vix_data.rename(columns={'Close': 'VIX_Level'}, inplace=True)

# Merge SPX and VIX data on 'Date' column
df = pd.merge(spx_data[['Date', 'SPX_Level']], vix_data[['Date', 'VIX_Level']], on='Date', how='outer')
df

# Calculate daily log returns for SPX and VIX
df['spx_log_returns'] = np.log(df['SPX_Level'] / df['SPX_Level'].shift(1))
df['vix_log_returns'] = np.log(df['VIX_Level'] / df['VIX_Level'].shift(1))

# Drop the first row with NaN values due to shift operation
df = df.iloc[1:]
df.head()
df

# Ascending order of date
df['Date'] = pd.to_datetime(df['Date'])
df = df.sort_values('Date', ascending = True)
df

# Visualize SPX and VIX Price Trend
plt.figure(figsize=(10, 6))

plt.subplot(2, 1, 1)
plt.plot(df['Date'], df['SPX_Level'], label='SPX Level', color='blue')
plt.title('SPX Trends')
plt.xlabel('')
plt.ylabel('Level')
plt.grid(True)


plt.subplot(2, 1, 2)
plt.plot(df['Date'], df['VIX_Level'], label='VIX Level', color='red')
plt.title('VIX Trends')
plt.xlabel('')
plt.ylabel('Level')
plt.grid(True)

plt.show()

# Visualize log returns distribution
plt.figure(figsize=(10, 6))
sns.histplot(df['spx_log_returns'], kde=True, bins=30, color='blue', label='SPX Log Returns')
sns.histplot(df['vix_log_returns'], kde=True, bins=30, color='red', label='VIX Log Returns')
plt.legend()
plt.title('Distribution of Log Returns')
plt.show()

"""# Part 2: Statistical Analysis of SPX and VIX Log Returns"""

# Descriptive statistics
mean_spx = df['spx_log_returns'].mean()  # Mean of SPX log returns
std_spx = df['spx_log_returns'].std()    # Standard deviation of SPX log returns
mean_vix = df['vix_log_returns'].mean()  # Mean of VIX log returns
std_vix = df['vix_log_returns'].std()    # Standard deviation of VIX log returns

# Check for missing values in log returns
missing_spx = df['spx_log_returns'].isnull().sum()
missing_vix = df['vix_log_returns'].isnull().sum()
if missing_spx > 0 or missing_vix > 0:
    print(f"Warning: Missing SPX values: {missing_spx}, Missing VIX values: {missing_vix}")

# Pearson correlation between SPX and VIX log returns
correlation = df['spx_log_returns'].corr(df['vix_log_returns'])

# Spearman correlation for non-linear relationships
spearman_corr, spearman_p = stats.spearmanr(df['spx_log_returns'], df['vix_log_returns'])

# Compute descriptive statistics for both SPX and VIX
descriptive_stats = df[['spx_log_returns', 'vix_log_returns']].describe()

# Print all statistics
print("### Descriptive Statistics ###\n")
print(f"SPX Mean: {mean_spx:.6f}, SPX Std Dev: {std_spx:.6f}")
print(f"VIX Mean: {mean_vix:.6f}, VIX Std Dev: {std_vix:.6f}")
print(f"Pearson Correlation: {correlation:.6f}")
print(f"Spearman Correlation: {spearman_corr:.6f}, P-value: {spearman_p:.6f}")
print("\nDescriptive Statistics Table:")
print(descriptive_stats)

"""# Part 3: Visualizing SPX and VIX Log Returns"""

# Scatter plot of SPX vs VIX Levels with Regression Line
  plt.figure(figsize=(10, 6))
  sns.regplot(x='SPX_Level', y='VIX_Level', data=df, scatter_kws={'alpha':0.5}, line_kws={'color':'blue'}, ci=None)
  plt.title('Scatter Plot of SPX Levels vs VIX Levels')
  plt.xlabel('SPX Levels')
  plt.ylabel('VIX Levels')
  plt.xlim(2300, 3000)
  plt.ylim(10, 40)
  plt.grid()
  plt.show()

# Scatter plot of SPX vs VIX log returns
  plt.figure(figsize=(10, 6))
  plt.scatter(df['spx_log_returns'], df['vix_log_returns'], alpha=0.5)
  plt.title('Scatter Plot of SPX Log Returns vs VIX Log Returns')
  plt.xlabel('SPX Log Returns')
  plt.ylabel('VIX Log Returns')
  plt.axhline(0, color='red', linestyle='--', linewidth=1)
  plt.axvline(0, color='red', linestyle='--', linewidth=1)
  plt.xlim(-0.05, 0.05)
  plt.grid()
  plt.show()

"""# Part 4: Modeling Market Scenarios with Copula

## Step 1: Create Rank Columns
"""

# Copy DataFrame to avoid modifying original data
df1 = df.copy()

# Generate rank columns for SPX and VIX log returns
df1['SPX Rank'] = df1['spx_log_returns'].rank()
df1['VIX Rank'] = df1['vix_log_returns'].rank()

df1

"""## Step 2: Compute Empirical Cumulative Distribution Function (eCDF)"""

# Calculate total number of observations in df1
n = len(df1)

# Calculate eCDF for SPX and VIX based on their ranks
df1.loc[:, 'SPX eCDF'] = df1['SPX Rank'] / (n + 1)
df1.loc[:, 'VIX eCDF'] = df1['VIX Rank'] / (n + 1)
df1

"""## Step 3: Verify Mean and Standard Deviation"""

# Use the previously computed values from Part 2
# mean_spx, std_spx, mean_vix, std_vix are already defined
print("Mean and Standard Deviation:")
print(f"SPX Mean: {mean_spx}, SPX Std Dev: {std_spx}")
print(f"VIX Mean: {mean_vix}, VIX Std Dev: {std_vix}")

"""## Step 4: Transform eCDF to Normal Distribution"""

# Transform eCDFs into normal distribution
df1.loc[:, 'SPX Normal'] = stats.norm.ppf(df1['SPX eCDF'], loc=mean_spx, scale=std_spx)
df1.loc[:, 'VIX Normal'] = stats.norm.ppf(df1['VIX eCDF'], loc=mean_vix, scale=std_vix)

# Display transformed normal distribution columns
df1

"""## Step 5: Create New DataFrame for Further Analysis"""

# Create a new DataFrame for eCDF and transformed normal values
df2 = df1[['SPX eCDF', 'VIX eCDF', 'SPX Normal', 'VIX Normal']]

# Display the new DataFrame
print("New DataFrame df2:")
print(df2.head())

"""## Step 6: Compute Excess Kurtosis and Degrees of Freedom"""

# Compute excess kurtosis for SPX and VIX log returns
excess_kurt_spx = kurtosis(df1['spx_log_returns'], bias=False)
excess_kurt_vix = kurtosis(df1['vix_log_returns'], bias=False)
print(f"Excess Kurtosis - SPX: {excess_kurt_spx}, VIX: {excess_kurt_vix}")

# Compute degrees of freedom based on excess kurtosis
df_spx = (6 / excess_kurt_spx) + 4
df_vix = (6 / excess_kurt_vix) + 4
print(f"Degrees of Freedom - SPX: {df_spx}, VIX: {df_vix}")

# Compute tau for scaling
tau_spx = np.sqrt((df_spx - 2) / df_spx) * std_spx
tau_vix = np.sqrt((df_vix - 2) / df_vix) * std_vix
print(f"Tau - SPX: {tau_spx}, VIX: {tau_vix}")

"""## Step 7: Transform eCDF to Student's t Distribution"""

df2 = df2.copy()

# Transform eCDF to Student's t distribution using inverse t-distribution
df2['SPX t Inv'] = stats.t.ppf(df1['SPX eCDF'], df=df_spx) * tau_spx + mean_spx
df2['VIX t Inv'] = stats.t.ppf(df1['VIX eCDF'], df=df_vix) * tau_vix + mean_vix

df2 = df2[['SPX eCDF', 'VIX eCDF', 'SPX Normal', 'VIX Normal', 'SPX t Inv', 'VIX t Inv']]
df2

# Add original log returns for comparison
df2['SPX Log Returns'] = df1['spx_log_returns']
df2['VIX Log Returns'] = df1['vix_log_returns']

# Display updated DataFrame
print("Updated DataFrame df2:")
df2.head()

"""## Step 8: Visualize SPX and VIX Transformations"""

# Scatter plot for SPX
plt.figure(figsize=(12, 8))
# SPX Log Returns vs SPX Log Returns (line)
plt.plot(df2['SPX Log Returns'], df2['SPX Log Returns'], color='gray', linestyle='--', label='Line')
# SPX Normal vs SPX Log Returns
plt.scatter(df2['SPX Normal'], df2['SPX Log Returns'], color='blue', alpha=0.6, label='SPX Normal vs Log Returns')
# SPX t Inv vs SPX Log Returns
plt.scatter(df2['SPX t Inv'], df2['SPX Log Returns'], color='red', alpha=0.6, label='SPX t Inv vs Log Returns')

plt.title('SPX Log Returns Comparison')
plt.xlabel('SPX Normal / SPX t Inv')
plt.ylabel('SPX Log Returns')
plt.legend()
plt.grid()
plt.show()

# Scatter plot for VIX
plt.figure(figsize=(12, 8))
# VIX Log Returns vs VIX Log Returns (line)
plt.plot(df2['VIX Log Returns'], df2['VIX Log Returns'], color='gray', linestyle='--', label='Line')
# VIX Normal vs VIX Log Returns
plt.scatter(df2['VIX Normal'], df2['VIX Log Returns'], color='blue', alpha=0.6, label='VIX Normal vs Log Returns')
# VIX t Inv vs VIX Log Returns
plt.scatter(df2['VIX t Inv'], df2['VIX Log Returns'], color='red', alpha=0.6, label='VIX t Inv vs Log Returns')

plt.title('VIX Log Returns Comparison')
plt.xlabel('VIX Normal / VIX t Inv')
plt.ylabel('VIX Log Returns')
plt.legend()
plt.grid()
plt.show()

"""## Step 9: Compute Correlation and Copula Parameters"""

# Pearson correlation between SPX and VIX log returns
correlation = df1['spx_log_returns'].corr(df1['vix_log_returns'])
print(f"Pearson Correlation: {correlation}")

# Spearman rho correlation
rho, p_value = stats.spearmanr(df1['spx_log_returns'], df1['vix_log_returns'])
print(f"Spearman's rho: {rho}, P-value: {p_value}")

# Copula rho transformation
copula_rho = 2 * math.sin(math.pi * rho / 6)
print(f"Copula Rho: {copula_rho}")

"""## Step 10: Create Cholesky Matrix for Dependency Structure"""

# Construct Cholesky decomposition matrix
cholesky_matrix = np.array([
    [1, copula_rho],
    [0, np.sqrt(1 - copula_rho ** 2)]
])

print("Cholesky Upper Triangular Matrix:")
print(cholesky_matrix)

"""## Step 11: Generate Standard Normal Random Numbers"""

# Set random seed for reproducibility
np.random.seed(42)

# Generate standard normal random numbers
data = np.random.normal(loc=0, scale=1, size=(1000, 2))
df2 = pd.DataFrame(data, columns=['Z1', 'Z2'])

# Display generated random numbers
print("Standard Normal Random Numbers:")
df2

"""## Step 12: Analyze Correlation and Variance of Generated Data"""

# Calculate correlation and variance for Z1 and Z2
corr_z1z2 = df2['Z1'].corr(df2['Z2'])
var_z1 = df2['Z1'].var()
var_z2 = df2['Z2'].var()

print("correlation", corr_z1z2)
print("var_z1", var_z1)
print("var_z2", var_z2)

"""## Step 13: Apply Cholesky Matrix Transformation"""

# Convert DataFrame into a NumPy matrix for transformation
matrix_z1z2 = df2[['Z1', 'Z2']].to_numpy()
matrix_z1z2

# Apply the Cholesky transformation to induce correlation based on the copula matrix
correl_st_normal = np.dot(matrix_z1z2, cholesky_matrix)
correl_st_normal

# Correlated Standard Normal Data
correl_st_normal = pd.DataFrame(correl_st_normal, columns=['norm_SPX', 'norm_VIX'])

print("Correlated Standard Normal Data:")
correl_st_normal

"""## Step 14: Convert the correlated data back into a DataFrame"""

# Concatenate the original random numbers (Z1, Z2) with the correlated results
simulation = pd.concat([df2[['Z1', 'Z2']], correl_st_normal[['norm_SPX', 'norm_VIX']]], axis=1)

print("Simulation DataFrame with Z1, Z2, norm_SPX, norm_VIX:")
simulation

"""## Step 15: Use correlated standard normal variables to simulate SPX and VIX values"""

# Simulate mean-variance normal SPX and VIX values
simulation['mv_norm_SPX'] = mean_spx + std_spx * simulation['norm_SPX']
simulation['mv_norm_VIX'] = mean_vix + std_vix * simulation['norm_VIX']

print("Multivariate Normal Simulated SPX and VIX:")
simulation

"""## Step 16: Simulate Uniform Variables
### Convert standard normal variables into uniform variables using Cumulative Distribution Function (CDF)
"""

# Simulated uniform SPX and VIX
simulation['uniform_SPX'] = norm.cdf(simulation['norm_SPX'])
simulation['uniform_VIX'] = norm.cdf(simulation['norm_VIX'])

print("Uniform Simulated SPX and VIX:")
simulation

"""## Step 17: Simulate Log Returns Using Copula Margins
### Convert uniform variables to log returns using inverse Student's t distribution
"""

# Simulated diff LN (norm copula with t margins)
simulation['diff_ln_SPX'] = mean_spx + tau_spx * t.ppf(simulation['uniform_SPX'], df_spx)
simulation['diff_ln_VIX'] = mean_vix + tau_vix * t.ppf(simulation['uniform_VIX'], df_vix)

print("Simulated Log Returns (SPX and VIX):")
simulation

"""## Step 18: Calculate Simulated Levels for SPX and VIX
### Use the last observed levels from the original data to simulate future levels

"""

# The last day of observed data
# Define the last observed date
last_day = '2019-03-25'

# Fetch the last observed SPX and VIX levels
last_spx_level = df.loc[df['Date'] == last_day, 'SPX_Level'].values[0]
last_vix_level = df.loc[df['Date'] == last_day, 'VIX_Level'].values[0]

# Simulate SPX and VIX levels by applying the exponential transformation
simulation['SPX_Level'] = last_spx_level * np.exp(simulation['diff_ln_SPX'])
simulation['VIX_Level'] = last_vix_level * np.exp(simulation['diff_ln_VIX'])

print("Final Simulated Levels for SPX and VIX:")
simulation

"""## Step 19: Plot Simulated Results"""

# Scatter plots of the simulated log returns
plt.figure(figsize=(10, 6))
plt.scatter(simulation['diff_ln_SPX'], simulation['diff_ln_VIX'], alpha=0.5, color='blue')
plt.title("Simulated Log Returns (SPX vs VIX)")
plt.xlabel("SPX Log Returns")
plt.ylabel("VIX Log Returns")
plt.grid()
plt.show()

# Scatter plots of the simulated levels
plt.figure(figsize=(10, 6))
plt.scatter(simulation['SPX_Level'], simulation['VIX_Level'], alpha=0.5, color='green')
plt.title("Scatter Plot of Simulated Levels (SPX vs VIX)")
plt.xlabel("SPX Level")
plt.ylabel("VIX Level")
plt.grid()
plt.show()

"""# Part 5: Options Pricing Using Black-Scholes Model"""

################ portfolio ################

"""## Step 1: Define the portfolio containing options information"""

portfolio = pd.DataFrame({
    'holdings': [100, -200, 100], # Number of options held
    'strike': [195, 200, 205],    # Strike prices for the options
})
portfolio

# Dates for valuation and expiration
portfolio['date'] = pd.to_datetime('2019-03-26')
portfolio['expiration_date'] = pd.to_datetime('2019-04-18')

# Calculate days to expiration
portfolio['days_to_expiration'] = (portfolio['expiration_date'] - portfolio['date']).dt.days.astype(int)
portfolio

"""## Step 2: Assign interest rate and implied volatility for each option"""

portfolio['rate'] = 0.0245  # Annual risk-free rate
portfolio['implied_volatility'] = pd.DataFrame([0.1665, 0.1634, 0.1447]) # Implied volatilities
portfolio['underlying_price'] = 200.62 # Current underlying price

portfolio

"""## Step 3: Calculate Black-Scholes Parameters"""

# Calculate time to expiration in years
portfolio['time'] = (portfolio['days_to_expiration']/365)

# Calculate the discounted factor (rate*time) and volatility factor
portfolio['rt'] = (portfolio['time']*portfolio['rate'])
portfolio['sigma_sqr_t'] = ((portfolio['implied_volatility']**2)*portfolio['time'])
portfolio

# Calculate d1 and d2 parameters
portfolio = portfolio.assign(
    d1=lambda df: (
        (np.log(df['underlying_price'] / df['strike']) +
         (df['rate'] + ((df['implied_volatility']**2) / 2)) * df['time']
        ) / (df['implied_volatility'] * np.sqrt(df['time']))
    )
)
portfolio[['d1']]

"""## Step 4: Calculate Option Prices Using Black-Scholes Formula

### Calculate N(d1) and N(d2) using the Cumulative Distribution Function (CDF)
"""

portfolio['d2'] = portfolio['d1']-portfolio['implied_volatility']*np.sqrt(portfolio['time'])
portfolio

portfolio['norm_d1'] = norm.cdf(portfolio['d1'])
portfolio['norm_d2'] = norm.cdf(portfolio['d2'])
portfolio

# Calculate Black-Scholes call option prices
portfolio = portfolio.assign(
    price=lambda df: (
        df['underlying_price']*df['norm_d1'] -
        df['strike']*np.exp((-1)*df['rt'])*df['norm_d2']
    )
)

# Calculate total market value for portfolio
portfolio['total'] = portfolio['holdings']*portfolio['price']

portfolio[['price', 'total']]

"""## Step 5: Combine Portfolio Data and Calculate Total Value

### Calculate the total value of the portfolio including initial adjustment
"""

### TOTAL VALUE ###
total_market_value = portfolio['total'].sum() + 20
print(f"Total Market Value of Portfolio: {total_market_value}")

"""## Step 6: Analyze Simulation Data"""

# Copy simulation data
bs = simulation[['diff_ln_SPX', 'diff_ln_VIX']].copy()
bs

"""### Strike 195

### Filter portfolio data for strike price 195
"""

strike_195 = portfolio[portfolio['strike'] == 195].copy()
strike_195

"""### Calculate relevant parameters for strike price 195"""

bs_195 = pd.DataFrame()
bs_195['S_0'] = np.exp(bs['diff_ln_SPX'])*strike_195['underlying_price'].values[0]
bs_195['volatility'] = np.exp(bs['diff_ln_VIX'])*strike_195['implied_volatility'].values[0]
bs_195['time'] = strike_195['time'].values[0]
bs_195['sigma_sqr_t'] = (bs_195['volatility']**2)*bs_195['time']
bs_195

"""### Calculate d1, d2, N(d1), N(d2), and option price for strike 195"""

bs_195 = bs_195.assign(
    d1=lambda df: (
        (np.log(df['S_0'] / strike_195['strike'].values[0]) +
         (strike_195['rate'].values[0] + ((df['volatility']**2) / 2)) * df['time']
        ) / (df['volatility'] * np.sqrt(df['time']))
    ),
    d2 = lambda df: (
        df['d1']-df['volatility']*np.sqrt(df['time'])
    ),
    norm_d1 = lambda df: (
        norm.cdf(df['d1'])
    ),
    norm_d2 = lambda df: (
        norm.cdf(df['d2'])
    ),
    call_1 = lambda df: (
        df['S_0']*df['norm_d1'] -
        strike_195['strike'].values[0]*np.exp(-strike_195['rate'].values[0]*df['time'])*df['norm_d2']
    )
)
bs_195

"""### Strike 200

### Filter portfolio data for strike price 200
"""

strike_200 = portfolio[portfolio['strike'] == 200].copy()
strike_200

"""### Calculate relevant parameters for strike price 200"""

bs_200 = pd.DataFrame()
bs_200['S_0'] = np.exp(bs['diff_ln_SPX'])*strike_200['underlying_price'].values[0]
bs_200['volatility'] = np.exp(bs['diff_ln_VIX'])*strike_200['implied_volatility'].values[0]
bs_200['time'] = strike_200['time'].values[0]
bs_200['sigma_sqr_t'] = (bs_200['volatility']**2)*bs_200['time']
bs_200

"""### Calculate d1, d2, N(d1), N(d2), and option price for strike 200"""

bs_200 = bs_200.assign(
    d1=lambda df: (
        (np.log(df['S_0'] / strike_200['strike'].values[0]) +
         (strike_200['rate'].values[0] + ((df['volatility']**2) / 2)) * df['time']
        ) / (df['volatility'] * np.sqrt(df['time']))
    ),
    d2 = lambda df: (
        df['d1']-df['volatility']*np.sqrt(df['time'])
    ),
    norm_d1 = lambda df: (
        norm.cdf(df['d1'])
    ),
    norm_d2 = lambda df: (
        norm.cdf(df['d2'])
    ),
    call_2=lambda df: (
        df['S_0'] * df['norm_d1'] -
        strike_200['strike'].values[0] * np.exp(-strike_200['rate'].values[0] * df['time']) * df['norm_d2']
    )
)
bs_200

"""### Strike 205

### Filter portfolio data for strike price 205
"""

strike_205 = portfolio[portfolio['strike'] == 205].copy()
strike_205

"""### Calculate relevant parameters for strike price 205"""

bs_205 = pd.DataFrame()
bs_205['S_0'] = np.exp(bs['diff_ln_SPX'])*strike_205['underlying_price'].values[0]
bs_205['volatility'] = np.exp(bs['diff_ln_VIX'])*strike_205['implied_volatility'].values[0]
bs_205['time'] = strike_205['time'].values[0]
bs_205['sigma_sqr_t'] = (bs_205['volatility']**2)*bs_205['time']
bs_205

"""### Calculate d1, d2, N(d1), N(d2), and option price for strike 205"""

bs_205 = bs_205.assign(
    d1=lambda df: (
        (np.log(df['S_0'] / strike_205['strike'].values[0]) +
         (strike_205['rate'].values[0] + ((df['volatility']**2) / 2)) * df['time']
        ) / (df['volatility'] * np.sqrt(df['time']))
    ),
    d2 = lambda df: (
        df['d1']-df['volatility']*np.sqrt(df['time'])
    ),
    norm_d1 = lambda df: (
        norm.cdf(df['d1'])
    ),
    norm_d2 = lambda df: (
        norm.cdf(df['d2'])
    ),
    call_3=lambda df: (
        df['S_0'] * df['norm_d1'] -
        strike_205['strike'].values[0] * np.exp(-strike_205['rate'].values[0] * df['time']) * df['norm_d2']
    )
)
bs_205

"""## Step 7: Combine Simulated Results"""

# Combine simulated call prices for all strikes
calls = pd.DataFrame({
    'call_1': bs_195['call_1'],
    'call_2': bs_200['call_2'],
    'call_3': bs_205['call_3']
})

# Calculate the total market value
calls['market_value'] = (
    calls['call_1'] * strike_195['holdings'].values[0] +
    calls['call_2'] * strike_200['holdings'].values[0] +
    calls['call_3'] * strike_205['holdings'].values[0] +
    20
)

# Calculate profit and loss (PnL) and return
calls['initial_value'] = total_market_value
calls['PnL'] = calls['market_value'] - calls['initial_value']
calls['return'] = calls['PnL'] / calls['initial_value']

calls

# market value, pnl, returns
mv_mean = calls['market_value'].mean()
mv_std = calls['market_value'].std()
mv_min = calls['market_value'].min()
mv_max = calls['market_value'].max()
pnl_mean = calls['PnL'].mean()
pnl_std = calls['PnL'].std()
return_mean = calls['return'].mean()
return_std = calls['return'].std()
call1_min = calls['call_1'].min()
call1_max = calls['call_1'].max()
call2_min = calls['call_2'].min()
call2_max = calls['call_2'].max()
call3_min = calls['call_3'].min()
call3_max = calls['call_3'].max()
print(f"market values mean: {mv_mean}")
print(f"market values std: {mv_std}")
print(f"market value range: {mv_min} {mv_max}")
print(f"pnl mean: {pnl_mean}")
print(f"pnl std: {pnl_std}")
print(f"return mean: {return_mean}")
print(f"return std: {return_std}")
print(f"call 1 range: {call1_min} {call1_max}")
print(f"call 2 range: {call2_min} {call2_max}")
print(f"call 3 range: {call3_min} {call3_max}")

"""## Step 8: Calculate Value at Risk (VaR)

### Calculate 5% Value at Risk (VaR) for profit & loss and returns
"""

percentile_value = 5
VaR_PnL = -np.percentile(calls['PnL'], percentile_value)
VaR_return = -np.percentile(calls['return'], percentile_value)

print(f"Value at Risk (PnL): {VaR_PnL}")
print(f"Value at Risk (Return): {VaR_return}")

"""## Step 9: Visualize Portfolio Market Values"""

# Cumulative percentage
market_values = calls['market_value']
bins = np.linspace(min(market_values), max(market_values), 61)
hist, _ = np.histogram(market_values, bins)
cumulative = np.cumsum(hist) / len(market_values) * 100

# Plot histogram and cumulative percentage
fig, ax1 = plt.subplots() # y-axis LHS
sns.histplot(market_values, bins=60, kde=False, stat='frequency', color='blue', alpha=0.7, ax=ax1)
ax1.set_xlabel('Portfolio Market Values')
ax1.set_ylabel('Frequency')
ax1.set_title('Portfolio Market Values')

# Secondary y-axis
ax2 = ax1.twinx()  # y-axis RHS
ax2.plot(bins[1:], cumulative, color='orange', linestyle='-', linewidth=1, marker='D', label='Cumulative Percentage (RHS)')
ax2.set_ylabel('Cumulative Percentage')

# Add legend
fig.legend(loc='upper left', labels=["Frequency", "Cumulative Percentage"])
plt.show()

"""#### The histogram and cumulative percentage plot visualize the distribution of simulated portfolio market values. The portfolioâ€™s most probable market value clusters around 100 to 110 units, with a cumulative percentage quickly reaching 100%. The butterfly spread strategy ensures a controlled risk profile, reflected in the narrow and symmetric distribution."""

